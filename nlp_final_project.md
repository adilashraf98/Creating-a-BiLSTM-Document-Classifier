# -*- coding: utf-8 -*-
"""NLP_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xC832YGge6XdUk6jA3LZHd1HLfA2FpMk

##Purpose and Introduction

The purpose of this code is to build document classification model using four different 
approaches - bag-of-words with Naive Bayes classifier and BiLSTM model with Elmo and BERT 
elbedding layer. The document classification is an important task in natural language 
processing as it allows to automatically assigning topics to news articles, books, 
social media post, etc. We decided to work with BBC dataset that has collection of 
news articles and classes. This dataset has five categories, however for the purpose 
of our project, we will be using only three

###Importing Dataset

This Python code uses the Pandas library to load a dataset in CSV format from a URL, 
specifically the BBC news dataset. The loaded dataset is stored in a Pandas DataFrame 
called df. The BBC dataset contains news articles published by the BBC news website, 
along with their corresponding category labels. The dataset consists of 2,225 documents 
in five categories: Business, Entertainment, Politics, Sport, and Tech.
"""

import pandas as pd

# Load the dataset
url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'
df = pd.read_csv(url)

# Print the first few rows of the dataset
print(df.head())

# Print the unique categories in the dataset
print(df['category'].unique())

"""This Python code calculates the average length of the news articles in the BBC dataset"""

import numpy as np

d = []
for t in df['text']:
  l = len(t)
  d.append(l)
print( sum(d) / len(d))

len(df)

"""The code first imports the NLTK library, which provides tools for working with natural 
language text in Python. Next, we download the ‘punkt’ which is pre-trained tokenizer. 
We use this tokenizer for our word_tokenize() function. Next, we create new column called 
‘tokenized_text” which will store the tokenized version of each article. Finally, the code 
applies word_tokenize() function to text colum. This function tokenizes each news article 
into a list of words, and the resulting list is stored in the tokenized_text column of the 
same row."""

import nltk
nltk.download('punkt')
df["tokenized_text"] = df["text"].apply(nltk.word_tokenize)

"""We use train_test_split function from the sklearn.model_selection module to split your 
dataset into a training set and a test set. The test_size parameter is set to 0.2, meaning 
that 20% of the dataset will be reserved for the test set and the remaining 80% will be 
used for the training set. The random_state parameter is set to 42, which ensures that 
the random selection of data points is consistent across multiple runs."""

from sklearn.model_selection import train_test_split

# Splitting the dataset into testing and training data sets
train, test = train_test_split(df, test_size=0.2, random_state=42)

"""This code snippet is a function called prep, which takes a Pandas DataFrame df and a 
column name column as input. The purpose of this function is to preprocess the text data 
in the specified column by performing the following steps:
Download necessary NLTK resources: It downloads the NLTK stopword list and WordNet 
lemmatizer data, which are used for preprocessing the text data.
Remove stopwords: It creates a set of English stopwords using the NLTK library and 
removes them from the text. Stopwords are common words that do not carry significant 
meaning and are often removed from the text data to reduce noise and improve computational efficiency.
Lemmatize words: The function applies the WordNetLemmatizer from the NLTK library to 
convert words in the text to their base or dictionary form, known as lemmas. This 
process helps in reducing the dimensionality of the data and grouping similar words together.
Convert words to lowercase: It converts all words in the text to lowercase. This 
step is performed to ensure that the text data is consistent and not affected by 
differences in letter case.
Remove punctuation: The function uses a regular expression to remove any punctuation 
marks from the words in the text. Removing punctuation marks helps in simplifying the 
text and reducing noise in the data.

"""

import re
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

# Doing the necessary pre-processing

def prep(df, column):
    stop_words = set(stopwords.words('english')) # creating a set of English stop words
    lemmatizer = WordNetLemmatizer() # creating a lemmatizer object
    df[column] = df[column].apply(lambda x: [word for word in x if word not in stop_words]) # remove stopwords
    df[column] = df[column].apply(lambda x: [lemmatizer.lemmatize(word) for word in x]) # lemmatizing each list
    df[column] = df[column].apply(lambda x: [word.lower() for word in x]) # converting each word to lowercase
    df[column] = df[column].apply(lambda x: [re.sub(r'[^\w\s]','', word) for word in x]) # remove punctuation
    return df

prep(df, 'tokenized_text')

df.head()

"""## Naive Bayes with Bag-of-Words Feature

For Naive Bayes classifier (with BOW approach) we slightly altered our pre-processing function. 
When dealing with stop words, we also accounted for negation words and did not remove them. 
Negations carry important sentiment and therefore, it is essential to preserve them.
"""

#Pre-processing for bag of words approach
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

def prep_bow(df, column):
    stop_words = set(stopwords.words('english')) - {'not', 'no', 'nor', 'neither'} # creating a set of English stop words excluding negation words
    lemmatizer = WordNetLemmatizer() # creating a lemmatizer object
    df[column] = df[column].apply(lambda x: [word for word in x if word not in stop_words]) # remove stopwords
    df[column] = df[column].apply(lambda x: [lemmatizer.lemmatize(word) for word in x]) # lemmatizing each list
    df[column] = df[column].apply(lambda x: [word.lower() for word in x]) # converting each word to lowercase
    df[column] = df[column].apply(lambda x: [re.sub(r'[^\w\s]','', word) for word in x]) # remove punctuation
    return df

prep_bow(df, 'tokenized_text')

#Correcting the order for easy use
docs = df[["tokenized_text","category"]]
docs = list(df[['tokenized_text', 'category']].itertuples(index=False, name=None))

len(docs)

"""Here, we create a baseline Bag-of-Words (BOW) model for text classification. 
First we import all necessary libraries. As step two we are shuffling the dataset 
so its randomly distributed before splitting in into training and testing sets. 
Step three: we create a a list of all words: all_words_list is a list of all words 
in the dataset. A frequency distribution of these words is calculated using nltk.FreqDist(). 
After that we selected 2000 most common words and stored them in word_features. 
Next, we defined document_feature() function which takes  a document and a list of word 
features as input, and it returns a dictionary with keys as 'V_word' and values as 
True or False, indicating whether the word is present in the document. 
Lastly, we generate feature sets: The featuresets variable is created by applying the 
document_features() function to each document in the docs list."""

#Baseline BOW
import nltk
import random

random.shuffle(docs)

all_words_list = [word for (sent,cat) in docs for word in sent]
all_words = nltk.FreqDist(all_words_list)

#Getting the most common 2000 words
word_items = all_words.most_common(2000)
word_features = [word for (word,count) in word_items]

def document_features(document, word_features):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['V_{}'.format(word)] = (word in document_words)
    return features

#get features sets for a document, including keyword features and category feature
featuresets = [(document_features(d, word_features), c) for (d, c) in docs]

# training using naive Baysian classifier, training set is approximately 90% of data
train_set, test_set = featuresets[1000:], featuresets[:1000]
classifier = nltk.NaiveBayesClassifier.train(train_set)

# evaluate the accuracy of the classifier
nltk.classify.accuracy(classifier, test_set)

# the accuracy result may vary since we randomized the documents

from sklearn.metrics import precision_recall_fscore_support


#Get the true labels and predicted labels for the test set
true_labels = [cat for (_, cat) in test_set]
predicted_labels = [classifier.classify(features) for (features, _) in test_set]

#Calculate precision, recall, and F1 score for each category
precision, recall, f1_score, _ = precision_recall_fscore_support(
    true_labels, predicted_labels, average='weighted')

#Print the results
print("Precision:", precision)
print("Recall:", recall)
print("F1 score:", f1_score)

"""This code is creating a text classification model using the bag-of-words approach 
and a Linear Support Vector Machine (SVM) classifier. The goal is to classify documents 
into categories based on the words they contain.
First, the documents are shuffled randomly to ensure a good distribution of categories 
in the training and testing datasets.
Then, a list of the most common 2000 words across all documents is created, which will 
serve as features for the classification model.
A function called document_features is defined to create a dictionary of word features 
for each document. This function checks if each word from the list of common words is 
present in a given document, and records this information as a feature.
The document_features function is applied to all documents to create a feature set, which 
is a collection of document features and their corresponding categories.
The feature set is split into a training set (200 documents) and a testing set (the remaining 
documents) to evaluate the performance of the model.
A Linear SVM classifier is trained on the training set using the SklearnClassifier 
class from nltk, which is a wrapper around scikit-learn's LinearSVC class.
Finally, the accuracy of the classifier is evaluated on the test set using nltk's 
classify.accuracy function, and the accuracy is printed out.
In summary, this code demonstrates how to create a simple text classification model 
by representing documents as bags of words and using a Linear SVM classifier to predict 
the categories of the documents. The accuracy of the model will depend on the dataset 
and the features used for classification.

"""

import random
from sklearn.svm import LinearSVC
from nltk.classify.scikitlearn import SklearnClassifier

# Shuffle the documents
random.shuffle(docs)

# Create bag of words feature set
all_words_list = [word for (sent,cat) in docs for word in sent]
all_words = nltk.FreqDist(all_words_list)
word_items = all_words.most_common(2000)
word_features = [word for (word,count) in word_items]
def document_features(document, word_features):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['V_{}'.format(word)] = (word in document_words)
    return features
featuresets = [(document_features(d, word_features), c) for (d, c) in docs]

# Split the data into training and testing sets
train_set, test_set = featuresets[200:], featuresets[:200]

# Train the SVM classifier
svm_classifier = SklearnClassifier(LinearSVC())
svm_classifier.train(train_set)

# Evaluate the accuracy of the classifier
accuracy = nltk.classify.accuracy(svm_classifier, test_set)
print('Accuracy:', accuracy)

import sklearn.metrics as metrics

# Get the true labels and predicted labels
true_labels = [cat for (doc, cat) in test_set]
predicted_labels = [svm_classifier.classify(doc) for (doc, cat) in test_set]

# Calculate the precision, recall, and F1 score
precision = metrics.precision_score(true_labels, predicted_labels, average='weighted')
recall = metrics.recall_score(true_labels, predicted_labels, average='weighted')
f1_score = metrics.f1_score(true_labels, predicted_labels, average='weighted')

# Print the results
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1_score)

"""**BiLSTM model using Elmo Embedding**"""

mask = (df['category'] == 'politics') | (df['category'] == 'tech') | (df['category'] == 'sport')
df = df[mask]

df = df[:300].copy()

df.head()

#Importing neccassary modules
# ! pip install "tensorflow>=1.7.0"
# ! pip install tensorflow-hub
import tensorflow as tf
import tensorflow_hub as hub

"""This code is importing a pre-trained machine learning model called ELMo, which can be 
used for processing natural language text. ELMo stands for "Embeddings from Language Models", 
and it's a deep neural network that can generate vector representations of words and sentences.
To use this pre-trained ELMo model, the code first disables a feature called "eager execution" 
in TensorFlow, which is a way to run TensorFlow operations immediately rather than creating a 
computational graph. Next, it clears the current backend session to reset the TensorFlow 
environment.
Then, the code initializes all the global variables in the TensorFlow graph to prepare for using 
the ELMo module. Finally, the code imports the ELMo model from the TensorFlow Hub, using a 
specific URL that points to version 2 of the ELMo model. The trainable=True parameter means 
that the weights of the ELMo model can be fine-tuned during training, allowing the model to 
learn from new data.
"""

#Importing pre-trained Elmo model
tf.compat.v1.disable_eager_execution()
tf.keras.backend.clear_session()
init = tf.compat.v1.global_variables_initializer()

elmo = hub.Module("https://tfhub.dev/google/elmo/2", trainable=True)

"""This code defines a function called embedding_elmo that can convert text input into ELMo 
embeddings. ELMo stands for "Embeddings from Language Models," and it's a type of machine 
learning model that can create numerical representations of words and sentences.
The function takes a text input x as its input. The elmo() function from TensorFlow Hub is 
used to process the input text and generate ELMo embeddings. The elmo() function takes a 
list of strings as input and returns the ELMo embeddings of each string as a tensor.
The next few lines of the function initialize the TensorFlow graph and session, which are 
necessary for using the ELMo module. The run() function is called twice to initialize the 
global variables and vocabulary lookup tables used by the ELMo module.
Finally, the reduce_mean() function is called to compute the average of the ELMo embeddings 
across the second dimension of the tensor. This returns a tensor with shape (batch_size, 
embedding_size), where batch_size is the number of input strings and embedding_size is the 
size of the ELMo embeddings. The function then returns this tensor as the output of the function.
Overall, this function is a useful tool for converting raw text into a format that can be 
used by machine learning models, especially those that are designed for natural language 
processing tasks like text classification or sentiment analysis.
Regenerate response

"""

def embedding_elmo(x):
  embeddings = elmo(x.tolist(), signature="default", as_dict=True)["elmo"]
  start = tf.compat.v1.global_variables_initializer()
  with tf.compat.v1.Session() as sess:
    sess.run(start)
    sess.run(tf.compat.v1.tables_initializer())
    # return average of ELMo features
    return sess.run(tf.reduce_mean(embeddings,1))

"""This code uses the train_test_split function from the sklearn.model_selection library 
to split our dataset df into separate training and testing subsets. The purpose of this 
split is to create a way to evaluate our machine learning model's performance on unseen data.
We decided to split the data into a training set and a testing set using a 80/20 split. 
This means that 80% of the data will be used for training the model, and 20% will be used 
for testing. We also set the random_state parameter to 42, which means that we will get 
the same split every time we run the code.
The output of the train_test_split function is a tuple of two dataframes: train and test. 
The train dataframe contains a subset of the original dataset that we will use to train our 
machine learning model. The test dataframe contains a subset of the data that we will use to 
evaluate the model's performance.
"""

from sklearn.model_selection import train_test_split

# Splitting the dataset into testing and training data sets
train, test = train_test_split(df, test_size=0.2, random_state=42)

train['category'].unique()

unique_categories = train['category'].unique().tolist()
print(unique_categories)

"""This code is dividing the train and test dataframes into smaller batches of size 
20 using a list comprehension. The smaller batches are created by iterating over the 
rows of the dataframes with a step size of 20 and selecting 20 rows starting from the 
current row index. The resulting batches are stored in two separate lists called list_tr 
and list_te."""

# Splitting the train and test datasets into smaller batches of size 20
list_tr = [train[i:i + 20] for i in range(0,train.shape[0], 20)]
list_te = [test[i:i + 20] for i in range(0,test.shape[0], 20)]

"""The embedding_elmo() function that we defined earlier is called for each batch of 
data, passing in the text data for each row of the batch. This function converts each 
text input into a tensor of ELMo embeddings.
The resulting ELMo embeddings for each batch of data are stored in two separate lists 
called elmo_train and elmo_test. These lists will be used as inputs to our machine 
learning model to train and test its performance.
"""

# Extracting ELMo embeddings
elmo_train = [embedding_elmo(x['text']) for x in list_tr]
elmo_test = [embedding_elmo(x['text']) for x in list_te]

import numpy as np

# Concatenating the arrays into one
elmo_train_new = np.concatenate(elmo_train, axis = 0)
elmo_test_new = np.concatenate(elmo_test, axis = 0)

elmo_train_new.shape

# Reshaping the data set for our model
elmo_train_new = elmo_train_new.reshape(elmo_train_new.shape[0], 1, elmo_train_new.shape[1])

"""This code is converting the categorical labels in the train and test dataframes 
into numerical labels using the pd.factorize() function from the Pandas library.
The factorize() function maps each unique value in a categorical column to a unique 
numerical label. The resulting labels are integers ranging from 0 to the number of unique categories minus 1.
The factorize() function is applied to the category column of both the train and test 
dataframes using the [0] index to select only the factorized labels, and then assigns the 
result back to the category column of each dataframe.
"""

train['category'] = pd.factorize(train['category'])[0]
test['category'] = pd.factorize(test['category'])[0]

# Importing the additional libraries
from tensorflow.keras import layers
from tensorflow.keras.layers import LSTM

from tensorflow.keras.layers import Input, Lambda, Bidirectional, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from keras.layers import Flatten

# Creating a sequential model
model_e = Sequential()

# Adding a Bidirectional LSTM layer with 128 units and return sequences
model_e.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(None, 1024)))

# Adding a dropout layer with rate 0.5
model_e.add(Dropout(0.5))

# Adding another Bidirectional LSTM layer with 64 units
model_e.add(Bidirectional(LSTM(units=64)))

# Adding another dropout layer with rate 0.5
model_e.add(Dropout(0.5))

# Adding a flatten layer
model_e.add(Flatten())

# Adding a dense layer with 1 unit and sigmoid activation
model_e.add(Dense(units=3, activation='softmax'))

# Compiling the model with adam optimizer and categorical crossentropy loss
model_e.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Fiting the model with training data and validation split of 0.2, for 10 epochs and batch size 32
model_e.fit(elmo_train_new, train['category'], batch_size=32, epochs=10, validation_split=0.2)

elmo_test_new.shape

# Reshaping the test set to use in the model
elmo_test_new = elmo_test_new.reshape(elmo_test_new.shape[0], 1, 1024)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

# Predicting the labels of the test set using the trained model
y_pred = model_e.predict(elmo_test_new)

y_pred

import numpy as np

# Define a list of category labels in the order that they appear in the y_pred array
test_labels = unique_categories

# Get the index of the maximum value in each row of the y_pred array
max_indices = np.argmax(y_pred, axis=1)

# Use the index to look up the corresponding category label in the list
predicted_labels = [test_labels[i] for i in max_indices]

# Print the predicted labels
for i, label in enumerate(predicted_labels):
    print("Sample {}: {}".format(i+1, label))

from sklearn.model_selection import train_test_split

# Splitting the dataset into testing and training data sets
train, test = train_test_split(df, test_size=0.2, random_state=42)

test['predicted_labels'] = predicted_labels
test.head(5)

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# Assigning 'text_positive' column from df to y_true
y_true = test['category']

# Assigning 'R_pos_count' column from merged to y_pred
y_pred = test['predicted_labels']

# Calculating confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plotting confusion matrix using matshow function from matplotlib
plt.matshow(cm, cmap=plt.cm.Blues)

# Adding colorbar to the plot
plt.colorbar()


# Adding labels to x-axis and y-axis of the plot
plt.xlabel("True labels")
plt.ylabel("Predicted labels ")


plt.show()

# Obtaining a score of how much do two data sets match
total_obs = np.sum(cm)
correct_classifications = np.diagonal(cm)
match_rate = np.sum(correct_classifications) / total_obs
print(match_rate)

"""##Testing BiLSTM + Elmo on the New Dataset

For prediction purposes we use News Category Dataset from Kaggle, which is a collection of 
news articles published over a period of three years from 2012 to 2015. The dataset includes 
over 200,000 articles from over 200 different news sources, and is often used for text 
classification and natural language processing tasks. The dataset has comparable labels, 
which makes it easy for us to check the accuracy of predictions
"""

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/

!kaggle datasets download -d rmisra/news-category-dataset

!kaggle datasets download -d rmisra/news-category-dataset

!unzip news-category-dataset.zip

import pandas as pd
df_f = pd.read_json('News_Category_Dataset_v3.json', lines=True)

#this code is selecting only the rows from the original dataframe that belong to the specified categories and creating a new dataframe with these rows
categories = ['POLITICS', 'ENTERTAINMENT', 'SPORTS', 'BUSINESS', 'TECH']
filtered_df = df_f[df_f['category'].isin(categories)]

filtered_df = filtered_df.drop(columns=['link', 'headline', 'authors', 'date']) #dropping uneccassary columns

filtered_df['category'] = filtered_df['category'].apply(lambda x: x.lower()) #lowercasing the data

filtered_df.reset_index(inplace=True)

filtered_df = filtered_df.drop(columns=['index'])

filtered_df.head(10)

# print(filtered_df[filtered_df['category'] == 'SPORT'])

#We are choosing 400 articles per tag

df_p = filtered_df[filtered_df['category'] == 'politics'][:400].copy()
df_t = filtered_df[filtered_df['category'] == 'tech'][:400].copy()
df_s = filtered_df[filtered_df['category'] == 'sports'][:400].copy()

df_p =  df_p[:400].copy()
df_t =  df_t[:400].copy()
df_s =  df_s[:400].copy()

len(df_s)

final = pd.concat([df_p, df_t, df_s])

final.head()

len(final)

def prep(df, column):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    df[column] = df[column].apply(lambda x: re.sub(r'[^\w\s]', '', x)) # remove punctuation
    df[column] = df[column].apply(lambda x: x.lower()) # convert text to lowercase
    df[column] = df[column].apply(lambda x: [word for word in nltk.word_tokenize(x) if word not in stop_words]) # tokenize and remove stopwords
    df[column] = df[column].apply(lambda x: [lemmatizer.lemmatize(word) for word in x]) # lemmatize each word
    df[column] = df[column].apply(lambda x: ' '.join(x)) # join the list of words back into a single string
    return df


prep(final, "short_description")



#In this code snippet, we are using the sample() method of the DataFrame with the following arguments:
#frac=1: This specifies that we want to sample the entire DataFrame, i.e., use all the rows.
#random_state=42: This sets the random seed to a fixed value, ensuring that the results are reproducible.
import random
shuffled_df = final.sample(frac=1, random_state=42)

shuffled_df.head()

lst = [shuffled_df[i:i + 100] for i in range(0,shuffled_df.shape[0], 100)]

elmo_f = [embedding_elmo(x['short_description']) for x in lst]

import numpy as np
elmo_f_new = np.concatenate(elmo_f, axis = 0)

elmo_f_new = elmo_f_new.reshape(elmo_f_new.shape[0], 1, elmo_f_new.shape[1])

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

# Predicting the labels of the test set using the trained model
y_pred = model_e.predict(elmo_f_new)

y_pred

# Define a list of category labels in the order that they appear in the y_pred array
final_labels = unique_categories
# Get the index of the maximum value in each row of the y_pred array
max_indices = np.argmax(y_pred, axis=1)

# Use the index to look up the corresponding category label in the list
predicted_labels = [final_labels[i] for i in max_indices]

# Print the predicted labels
for i, label in enumerate(predicted_labels):
    print("Sample {}: {}".format(i+1, label))

shuffled_df['predicted_labels'] = predicted_labels

shuffled_df.head()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# Assigning 'text_positive' column from df to y_true
y_true = shuffled_df['category']

# Assigning 'R_pos_count' column from merged to y_pred
y_pred = shuffled_df['predicted_labels']

# Calculating confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Obtaining a score of how much do two data sets match
total_obs = np.sum(cm)
correct_classifications = np.diagonal(cm)
match_rate = np.sum(correct_classifications) / total_obs
print(match_rate)

"""##BiLSTM + BERT

"""

!pip install transformers
!pip install keras --upgrade

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['label'] = le.fit_transform(df['category'])
df.head()

print(df['label'].unique())

df = df.iloc[:500]

"""The function generate_bert_embeddings takes a list of texts, a maximum sequence length, a 
tokenizer, and a BERT model as inputs. It encodes the texts using the tokenizer, truncates 
them if they exceed the maximum sequence length, pads them to the maximum sequence length, 
and generates embeddings using the BERT model. The embeddings are returned as a tensor."""

from tensorflow.keras.preprocessing.sequence import pad_sequences


def generate_bert_embeddings(texts, max_seq_length, tokenizer, model):
    """
    Generates BERT embeddings for a list of texts using a given tokenizer and model.
    Returns the embeddings as a tensor.
    """

    # Encode the texts using the tokenizer and truncate if they exceed the maximum sequence length
    encoded_texts = [tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, truncation=True) for text in texts]

    # Pad the encoded texts to the maximum sequence length
    padded_texts = pad_sequences(encoded_texts, maxlen=max_seq_length, padding='post', truncating='post')

    # Generate the embeddings using the BERT model
    embeddings = model(padded_texts)[0]

    return embeddings

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, SpatialDropout1D, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, TFBertModel
import numpy as np

# Split the data into training and testing sets
train_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

# Load the BERT tokenizer and model
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

# Generate BERT embeddings for the texts
max_seq_length = 100
train_embeddings = generate_bert_embeddings(train_texts, max_seq_length, bert_tokenizer, bert_model)
test_embeddings = generate_bert_embeddings(test_texts, max_seq_length, bert_tokenizer, bert_model)

num_classes = len(set(train_labels))

# Define the BiLSTM model architecture
model = Sequential([
    Bidirectional(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2, input_shape=(max_seq_length, 768))),
    Dense(units=32, activation='sigmoid'),
    Dropout(0.2),
    Dense(units=num_classes, activation='softmax')
])

#experimented with relu and sigmoid
#relu generated an accuracy of about 93% whereas the sigmoid function yielded an accuracy of 97% so we keep that

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])
#the learning rate seems to be optimal because we achieve similar accuracies on both our training, validation, and testing set.

# Train the model
batch_size = 32
num_epochs = 10
history = model.fit(train_embeddings, train_labels, batch_size=batch_size, epochs=num_epochs, validation_data=(test_embeddings, test_labels))

# Evaluate the model on the test data
loss, accuracy = model.evaluate(test_embeddings, test_labels)
print('Test loss:', loss)
print('Test accuracy:', accuracy)

model.save('my_model.h5')

shuffled_df.head()

def label_categories(category):
    if category == 'tech':
        return 0
    elif category == 'politics':
        return 1
    elif category == 'sports':
        return 2

shuffled_df['label'] = shuffled_df['category'].apply(label_categories)

#bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
#bert_model = TFBertModel.from_pretrained('bert-base-uncased')
max_seq_length = 100
df_subset = shuffled_df.head(100)

df_subset['embeddings'] = df_subset['short_description'].apply(lambda x: generate_bert_embeddings([x], max_seq_length, bert_tokenizer, bert_model))

from tensorflow.keras.models import load_model

# Load the saved BiLSTM model
bilstm_model = load_model('my_model.h5')

# Apply the model to each embedding in the "embeddings" column
df_subset['pred'] = df_subset['embeddings'].apply(lambda x: np.argmax(bilstm_model.predict(x), axis=-1))

df_subset = df_subset.drop('embeddings', axis=1)

df_subset.head(25)

"""##Conclusion and Challanges

Challenges:
One of the primary challenges we faced when training our Bidirectional LSTM (BiLSTM) model 
was limited computational power. It is well-known that large datasets of several thousand examples 
are typically necessary for training BiLSTM models effectively for natural language processing 
tasks, such as text classification or sentiment analysis.

Despite this recommendation, we were only able to use a small dataset of 300 articles due to 
computational constraints. This limited dataset resulted in a significant drop in model performance 
on new, unseen data compared to its performance on the validation and test sets. While the model achieved 
an accuracy of above 90% on the validation and test sets, its accuracy dropped to around 50% when evaluated on new data.

Based on our analysis of the results, it appears that the poor performance of the model on new, 
unseen data can be attributed to overfitting. The small dataset size likely resulted in the model 
becoming too specialized to the training data, and as a result, was unable to generalize well to new data.

We also ruled out any problems with biased datasets since we curated the dataset to be representative 
of different classes in equal amounts.

To mitigate the overfitting issue, we attempted to experiment with different combinations of dropout 
rates and epoch sizes, but were unable to identify a set of hyperparameters that improved the model's performance. 
Consequently, we recommend that researchers and practitioners take into account the size of the dataset 
when training BiLSTM models, as a larger dataset is likely to yield more accurate results.

In summary, while smaller datasets can still produce useful results, a sufficiently large dataset is 
recommended for training BiLSTM models to achieve optimal performance. This can help avoid the issue of 
overfitting and ensure that the model generalizes well to new, unseen data.
"""